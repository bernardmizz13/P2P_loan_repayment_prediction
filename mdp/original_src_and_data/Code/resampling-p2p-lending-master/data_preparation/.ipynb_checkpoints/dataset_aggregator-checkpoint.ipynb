{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2P Lending Dataset Preparation\n",
    "\n",
    "##### Luis Eduardo Boiko Ferreira,  PPGIa - PUCPR, luiseduardo.boiko@ppgia.pucpr.br\n",
    "##### Jean Paul Barddal,  PPGIa - PUCPR, jean.barddal@ppgia.pucpr.br\n",
    "##### Heitor Murilo Gomes, INFRES - Institut Mines-Télécom, heitor.gomes@telecom-paristech.fr\n",
    "##### Fabrício Enembreck, PPGIa - PUCPR, fabricio@ppgia.pucpr.br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script has the goal of merging the data made available from [Lending Club](https://www.lendingclub.com) between 2007 and 2016.\n",
    "In this work, we tackle only \"Charged Off\" and \"Fully Paid\" loans.\n",
    "The main steps taken to prepare the dataset are the following:\n",
    "\n",
    "1. Data load and header sanity check\n",
    "2. Data filter (charged off and fully paid) and concatenation\n",
    "3. Removal of features to avoid data leakage\n",
    "4. Removal and treatment of string variables\n",
    "5. Removal of instances (loan requests) with many missing values\n",
    "6. Removal of features (attributes) with many missing values\n",
    "7. Removal of variables of low variability\n",
    "8. Missing values imputation\n",
    "\n",
    "It is also important to mention that this script prepares two different versions of the dataset.\n",
    "The first is outputted between steps #4 and #5, and the other after step #8.\n",
    "The idea is to verify if the pre-processing that takes place between steps #5 and #8 impact somehow the learning algorithms and sampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data load and header sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Loads all datasets\n",
    "df2007to2011 = pd.read_csv(\"./LoanStats3a_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2012to2013 = pd.read_csv(\"./LoanStats3b_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2014       = pd.read_csv(\"./LoanStats3c_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2015       = pd.read_csv(\"./LoanStats3d_securev1.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q1     = pd.read_csv(\"./LoanStats_securev1_2016Q1.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q2     = pd.read_csv(\"./LoanStats_securev1_2016Q2.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q3     = pd.read_csv(\"./LoanStats_securev1_2016Q3.csv\", low_memory=False, skiprows=[0])\n",
    "df2016Q4     = pd.read_csv(\"./LoanStats_securev1_2016Q4.csv\", low_memory=False, skiprows=[0])\n",
    "df2017Q1     = pd.read_csv(\"./LoanStats_2017Q1.csv\", low_memory=False, skiprows=[0])\n",
    "df2017Q2     = pd.read_csv(\"./LoanStats_2017Q2.csv\", low_memory=False, skiprows=[0])\n",
    "\n",
    "all_dfs = [df2007to2011, df2012to2013, df2014, df2015, df2016Q1, df2016Q2, df2016Q3, df2016Q4, df2017Q1, df2017Q2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking out how the data is shaped and if they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42538, 128)\n",
      "(188183, 128)\n",
      "(235631, 128)\n",
      "(421097, 128)\n",
      "(133889, 128)\n",
      "(97856, 128)\n",
      "(99122, 128)\n",
      "(103548, 128)\n",
      "(96781, 137)\n",
      "(105453, 137)\n",
      "Subfiles are not maching!\n"
     ]
    }
   ],
   "source": [
    "columnsFirstDF = list(all_dfs[0].columns.values)\n",
    "error = False\n",
    "for df in all_dfs:\n",
    "    print(df.shape)\n",
    "    if set(df.columns.values) != set(columnsFirstDF):\n",
    "        error = True\n",
    "\n",
    "if error:\n",
    "    print(\"Subfiles are not maching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data filter (charged off and fully paid) and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587134, 143)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(all_dfs)\n",
    "#### Replaces 'loan_status' \"'Does not meet the credit policy. Status:Charged Off' 'Current'\" \n",
    "#### and \"'Does not meet the credit policy. Status:Fully Paid'\"\n",
    "df['loan_status'] = df['loan_status'].replace(\"'Does not meet the credit policy. Status:Charged Off' 'Current'\",\n",
    "                                              \"Charged Off\")\n",
    "df['loan_status'] = df['loan_status'].replace(\"'Does not meet the credit policy. Status:Fully Paid'\", \n",
    "                                              \"Fully Paid\")\n",
    "\n",
    "\n",
    "\n",
    "#### Filters dataset to contain only \"Charged Off\" and \"Fully Paid\" loans\n",
    "df = df.loc[(df.loan_status == \"Charged Off\") | (df.loan_status == \"Fully Paid\")]\n",
    "\n",
    "\n",
    "#### Converts the class to 0s and 1s\n",
    "df['loan_status'] = df['loan_status'].replace(\"Charged Off\", 1)\n",
    "df['loan_status'] = df['loan_status'].replace(\"Fully Paid\", 0)\n",
    "\n",
    "#### Converts this column to numbers, just in case.\n",
    "df['loan_status'] = pd.to_numeric(df['loan_status'], errors='ignore')\n",
    "\n",
    "df.set_index('id', inplace = True)\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "#### There are some missing values listed as a string \n",
    "#### in the 'n/a' format, so let's replace these for treatment later\n",
    "df.replace('n/a', np.nan, inplace = True)\n",
    "\n",
    "\n",
    "#### Sorts the dataset for the sake of visualization purposes \n",
    "df.sort_index(inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Removal of features to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Here, we keep only the variables listed in the dictionary file\n",
    "\n",
    "featuresToKeep = ['acc_open_past_24mths','addr_state','annual_inc', \n",
    "                 'annual_inc_joint','application_type','avg_cur_bal', \n",
    "                 'bc_open_to_buy','bc_util','chargeoff_within_12_mths', \n",
    "                 'collections_12_mths_ex_med','delinq_2yrs','delinq_amnt',\n",
    "                 'dti','dti_joint','earliest_cr_line','emp_length', \n",
    "                 'fico_range_high','fico_range_low','grade','home_ownership', \n",
    "                 'initial_list_status','inq_last_6mths','installment',\n",
    "                 'int_rate', 'loan_amnt','mths_since_last_delinq', \n",
    "                 'mths_since_last_major_derog','mths_since_last_record', \n",
    "                 'num_accts_ever_120_pd','open_acc','pub_rec', \n",
    "                 'pub_rec_bankruptcies','revol_bal','revol_bal_joint', \n",
    "                 'revol_util','sec_app_chargeoff_within_12_mths', \n",
    "                 'sec_app_collections_12_mths_ex_med','sec_app_earliest_cr_line', \n",
    "                 'sec_app_fico_range_high','sec_app_fico_range_low', \n",
    "                 'sec_app_inq_last_6mths','sec_app_mort_acc', \n",
    "                 'sec_app_mths_since_last_major_derog', \n",
    "                 'sec_app_num_rev_accts','sec_app_open_acc', \n",
    "                 'sec_app_open_il_6m','sec_app_revol_util', \n",
    "                 'sub_grade','tax_liens','term','tot_hi_cred_lim', \n",
    "                 'total_acc','total_bal_ex_mort','total_bc_limit', \n",
    "                 'total_il_high_credit_limit','total_rev_hi_lim', \n",
    "                 'verification_status', 'loan_status']\n",
    "\n",
    "df = df[featuresToKeep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Removal and treatment of string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up numeric data...\n",
      "Applying one hot encoding...\n",
      "Cleaning up date data...\n",
      "The new shape is now (587134, 161)\n"
     ]
    }
   ],
   "source": [
    "df_string = df.select_dtypes(exclude=[np.number])\n",
    "# print(df_string.shape)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "# display(df_string.head(1))\n",
    "\n",
    "print(\"Cleaning up numeric data...\")\n",
    "#### Converts some features to numeric\n",
    "def convertToNumeric(dataframe, list_of_attributes):\n",
    "    for f in list_of_attributes:\n",
    "        dataframe[f].replace(regex = True, inplace=True, to_replace=r'[^\\d.]+', value = r'')\n",
    "        dataframe[f] = pd.to_numeric(dataframe[f], errors='ignore')\n",
    "\n",
    "features_to_convert_to_numeric = ['term', \n",
    "                                  'revol_util', \n",
    "                                  'int_rate']\n",
    "convertToNumeric(df, features_to_convert_to_numeric)\n",
    "\n",
    "\n",
    "print(\"Applying one hot encoding...\")\n",
    "#### Applies one-hot-encoding to categorical variables\n",
    "def oneHotEncoding(dataframe, columnsToEncode):\n",
    "    new_dummies = []\n",
    "    for feature in columnsToEncode:\n",
    "        # creates dummies\n",
    "        dummies = pd.get_dummies(dataframe[feature], prefix=feature, prefix_sep='_')\n",
    "        for v in dummies.columns.values:\n",
    "            new_dummies.append(v)\n",
    "        # drops the feature\n",
    "        dataframe.drop(feature, axis = 1, inplace = True)\n",
    "        # appends n-1 features (the last is not necessary)\n",
    "        dummies.drop(dummies.columns[len(dummies.columns)-1], axis = 1, inplace=True)\n",
    "        dataframe = dataframe.join(dummies)\n",
    "    return dataframe, new_dummies\n",
    "\n",
    "categorical_features = ['grade', \n",
    "                        'sub_grade', \n",
    "                        'emp_length', \n",
    "                        'home_ownership', \n",
    "                        'verification_status', \n",
    "                        'addr_state', \n",
    "                        'initial_list_status', \n",
    "                        'application_type']\n",
    "df, new_dummies = oneHotEncoding(df, categorical_features)\n",
    "\n",
    "print(\"Cleaning up date data...\")\n",
    "#### TREATS DATE COLUMNS\n",
    "from datetime import datetime\n",
    "def separateDates(dataframe, columns):\n",
    "    for f in columns:\n",
    "        dataframe[f] = pd.to_datetime(dataframe[f], format='%b-%Y')\n",
    "        year = dataframe[f].apply(lambda x: x.strftime('%Y') if not pd.isnull(x) else '')\n",
    "        month = dataframe[f].apply(lambda x: x.strftime('%m') if not pd.isnull(x) else '')    \n",
    "        dataframe.drop(f, axis = 1, inplace = True)\n",
    "        df[(f + '_month')] = month\n",
    "        df[(f + '_year')] = year\n",
    "        df[(f + '_month')] = pd.to_numeric(df[(f + '_month')])\n",
    "        df[(f + '_year')] = pd.to_numeric(df[(f + '_year')])        \n",
    "    return df\n",
    "\n",
    "date_columns = ['earliest_cr_line']\n",
    "# all of these dates are in the mmm-YYYY format\n",
    "# and we wish to break them down into two separate columns: mm and YYYY\n",
    "df = separateDates(df, date_columns)\n",
    "\n",
    "print(\"The new shape is now {}\".format(df.shape))\n",
    "# display(df.head(1))\n",
    "# print(new_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's output the dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./p2p_lendingclub.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Removal of instances (loan requests) with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587134, 161)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Getting rid of instances with too many missing values (above 90%)\n",
    "df.dropna(thresh = 0.5 * df.shape[1], axis = 0, inplace = True)\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Removal of features (attributes) with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587134, 143)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Removes all features with more than 50% of the values missing\n",
    "df.dropna(thresh = 0.5 * df.shape[0], axis = 1, inplace = True)\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Removal of variables of low variability (below 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chargeoff_within_12_mths has a variability of 0.007211988866896779\n",
      "collections_12_mths_ex_med has a variability of 0.011170576993176362\n",
      "delinq_2yrs has a variability of 0.1806640392142168\n",
      "delinq_amnt has a variability of 0.0031491959246100487\n",
      "num_accts_ever_120_pd has a variability of 0.2276751299092029\n",
      "pub_rec has a variability of 0.15255801912340283\n",
      "pub_rec_bankruptcies has a variability of 0.11720440558832401\n",
      "tax_liens has a variability of 0.024428755141842506\n",
      "term has a variability of 0.2388074953928745\n",
      "(587134, 134)\n"
     ]
    }
   ],
   "source": [
    "toRemove = []\n",
    "for attribute in df.columns.values:\n",
    "    if attribute != 'loan_status' and attribute not in new_dummies:\n",
    "        count = pd.Series.value_counts(df[attribute])\n",
    "        maxCount = np.max(count)\n",
    "        variability = 1.0 - (float(maxCount) / count.sum())\n",
    "        if variability < .25:\n",
    "            print(\"{} has a variability of {}\".format(attribute, variability))            \n",
    "            toRemove.append(attribute)\n",
    "            \n",
    "            \n",
    "for f in toRemove:\n",
    "    df.drop(f, axis = 1, inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Missing values imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587134, 134)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>installment</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>...</th>\n",
       "      <th>addr_state_SC</th>\n",
       "      <th>addr_state_SD</th>\n",
       "      <th>addr_state_TN</th>\n",
       "      <th>addr_state_TX</th>\n",
       "      <th>addr_state_UT</th>\n",
       "      <th>addr_state_VA</th>\n",
       "      <th>addr_state_VT</th>\n",
       "      <th>addr_state_WA</th>\n",
       "      <th>addr_state_WI</th>\n",
       "      <th>addr_state_WV</th>\n",
       "      <th>initial_list_status_f</th>\n",
       "      <th>application_type_DIRECT_PAY</th>\n",
       "      <th>application_type_INDIVIDUAL</th>\n",
       "      <th>earliest_cr_line_month</th>\n",
       "      <th>earliest_cr_line_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>...</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "      <td>469685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>...</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "      <td>117449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             acc_open_past_24mths  annual_inc  avg_cur_bal  bc_open_to_buy  \\\n",
       "loan_status                                                                  \n",
       "0                          469685      469685       469685          469685   \n",
       "1                          117449      117449       117449          117449   \n",
       "\n",
       "             bc_util     dti  fico_range_high  fico_range_low  inq_last_6mths  \\\n",
       "loan_status                                                                     \n",
       "0             469685  469685           469685          469685          469685   \n",
       "1             117449  117449           117449          117449          117449   \n",
       "\n",
       "             installment  int_rate  loan_amnt  open_acc  revol_bal  \\\n",
       "loan_status                                                          \n",
       "0                 469685    469685     469685    469685     469685   \n",
       "1                 117449    117449     117449    117449     117449   \n",
       "\n",
       "             revol_util          ...            addr_state_SC  addr_state_SD  \\\n",
       "loan_status                      ...                                           \n",
       "0                469685          ...                   469685         469685   \n",
       "1                117449          ...                   117449         117449   \n",
       "\n",
       "             addr_state_TN  addr_state_TX  addr_state_UT  addr_state_VA  \\\n",
       "loan_status                                                               \n",
       "0                   469685         469685         469685         469685   \n",
       "1                   117449         117449         117449         117449   \n",
       "\n",
       "             addr_state_VT  addr_state_WA  addr_state_WI  addr_state_WV  \\\n",
       "loan_status                                                               \n",
       "0                   469685         469685         469685         469685   \n",
       "1                   117449         117449         117449         117449   \n",
       "\n",
       "             initial_list_status_f  application_type_DIRECT_PAY  \\\n",
       "loan_status                                                       \n",
       "0                           469685                       469685   \n",
       "1                           117449                       117449   \n",
       "\n",
       "             application_type_INDIVIDUAL  earliest_cr_line_month  \\\n",
       "loan_status                                                        \n",
       "0                                 469685                  469685   \n",
       "1                                 117449                  117449   \n",
       "\n",
       "             earliest_cr_line_year  \n",
       "loan_status                         \n",
       "0                           469685  \n",
       "1                           117449  \n",
       "\n",
       "[2 rows x 133 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Numeric features are imputed with the median, \n",
    "#### while categorical features are imputed with the mode\n",
    "for f in df.columns.values:\n",
    "    if df[f].dtype == np.float64 or df[f].dtype == np.int64:\n",
    "        df[f].fillna(df[f].median(),inplace = True)\n",
    "    else:\n",
    "        df[f].fillna(df[f].value_counts().index[0], inplace = True)\n",
    "print (df.shape)\n",
    "\n",
    "df.groupby(['loan_status']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves this final DF to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./p2p_lendingclub_filtered.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
